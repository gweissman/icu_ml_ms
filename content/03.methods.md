## Methods

We compared the predictive performance of manual versus automated modeling strategies across different approaches for split-sampling, handling of missing data, and the use of temporal data trends.
Individual investigators were responsible for implementing the manual (GW), automated (TL), and automated with temporal features (WL) modeling approaches in a competition-style format.
Only one investigator (SB) had access to the outcomes in the testing dataset for evaluation until after all models had been trained.

### Population and Data Collection

We used a dataset derived from a prospective cohort study that was conducted from 2013 to 2014 among patients who spent at least three days in an intensive care unit (ICU).[@Detsky_2017]
Among 303 patients in the original cohort, 301 (99.3%) had sufficient identifiers to be linked to their original chart in the electronic health record (EHR) to query detailed clinical data.
Interviews with patients and their surrogate decision makers were used to determine their quality of life prior to ICU admission.
Mortality status at six months after discharge was determined with review of the EHR and follow-up phone calls with patients and surrogates.

### Outcomes

For the primary analysis, each modeling approach was used to predict mortality after six months from hospital discharge.
In a secondary analysis, each modeling approach was used to predict the patient's quality of life, defined as a binary variable of whether the quality of life was at least as good as it was prior to the ICU admission.

### Model Types

#### Manual machine learning

We used the scikit-learn software package in Python to train a traditional multivariable logistic regression model, a penalized regression model (L1 and L2 penalties), and an XGBoost classification model.[@scikit-learn]
Because of the small sample sizes and relatively large number of features, each model was trained using the first 20 principle components of each training dataset.
In all cases, the same decomposition was used for the training and testing datasets.
Tuning parameters for the penalized regression and the XGBoost model were determined by grid search with 5-fold cross validation.

#### Automated Machine Learning

To test the performance of models developed through an automated machine learning approach, we used the Tree-based Pipeline Optimization Tool (TPOT). TODO(Trang)

#### Feat

To test the performance of an automated machine learning pipeline that could ... TODO(Bill)

### Missing Data

A manual chart review of the EHR confirmed that none of the missing data elements were due to an error in the dataset or database query, but were rather due to data not entered into the EHR.
We employed three different approaches to handling missing data to understand their effects on model performance in small clinical datasets and to see how they were related to performance using different modeling approaches.

First, we left all data as missing and allowed each modeling approach to deal with the data differently.
For the manually trained models, missingness indicator variables were generated for SBP, pH, albumin, and FiO2, considering that their absence would be informative based on clinical experience caring for patients in the ICU.
The remaining missing data were imputed using a k-nearest neighbors procedure. For the TPOT models, median imputation was used for all missing variables. 
For the Feat models, TODO(Bill).

Second, we pre-imputed all missing data so that all modeling approaches used the same imputed dataset.
Imputation in this case was performed with the mice package in R using Bayesian linear regression.[@r_mice]

Third, we performed a complete case analysis by entirely excluding the 8 most missing variables, then removing observations that had any missingness among the remaining variables.

### Split Sampling

The data were divided into training and testing samples using two different strategies.
The first strategy used 80% and 20% splits for training and testing, respectively. The second used 50% and 50% splits.
Split sampling for both strategies was performed with balanced stratification on ICU type (medical and non-medical) and by quartile of the APACHE score.
The observations were sampled such that the 20% test set is a subset of the 50% test set.

### Days of data

Numerous ICU mortality prediction models use data from the first 24 hours following admission.
Therefore, we aggregated available laboratory values and vital signs from the first 24 hours of the ICU admission in which each patient was enrolled in the initial study.

However, the trajectory of a patient's illness is sometimes not identifiable within the first 24 hours. It is unknown to what degree such temporal data, if at all, improves predictions of long-term outcomes. Therefore, we included an additional set of models with data from both the first and second 24-hour periods of the ICU admission.

For the manually created models, the difference between the two time periods was calculated for SBP, WBCs, FiO2, and UOP. These variables were chosen based on clinical experience as potentially relevant for determining a patient's trajectory. For TPOT and Feat, other features were ... TODO(Trang), TODO(Bill)

### Model Performance

We evaluated the predictive performance of each model using the scaled Brier Score ($BS_s$) as a measure that captures both discrimination and calibration.[@steyerberg2010assessing] Over $N$ predicted probabilities $p$ for some binary outcome $y$, the Brier Score is defined as

$$
BS = \frac{1}{N}\sum_i^N (y_i - p_i)^2.
$$

However, a useful prediction model should do better than just guessing the baseline event rate as a probability, and so scaling the Brier Score to this uninformed guess motivates using the scaled Brier Score such that

$$
BS_s = 1 - \frac{\frac{1}{N}\sum_i^N (y_i - p_i)^2}{\frac{1}{N}\sum_i^N (y_i - \bar y)^2}.
$$

A Scaled Brier Score of zero indicates that the model is equivalent to guessing the baseline event rate for each observation, a negative score indicates that the model is worse than this, and a positive score indicates that the model is better.

We generated confidence intervals around each performance estimate by calculating the Scaled Brier Score from 1,000 bootstrapped replicates of the predictions and observations for each model.
Differences in performance between models were calculated by estimating the boostrapped differences using 1,000 replicates.

### Computational resources

We calculated the time it took to train all models of each type using desktop hardware and utilizing parallel processing when available.