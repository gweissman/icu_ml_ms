## Introduction

A surge of interest in predictive modeling techniques has paralleled the increasing availability of large data sets and open source software packages that allow nearly out-of-the-box model development.
The popularity of and potential for data science methods is particularly relevant to the health care setting where decision making under uncertainty with large and varied data inputs are the daily norm.
However, many such modeling approaches have failed to yield evidence for their superiority over traditional statistical methods.[@doi:10.1016/j.jclinepi.2019.02.004]
One important reason for this lack of evidence for clinical prediction models is that large, observational datasets — such as those derived from electronic health records or administrative claims databases — are, despite being large and readily available, still fraught with selection bias and omitted variables, and frequently collected without the interests of the predictive analyst in mind.
Prospectively collected and clinically rich datasets with relevant, patient-centered outcomes are more rare.
When such datasets do exist, they tend to be much smaller than large EHR or claims databases because of the higher resources required to develop them.

Therefore, we turn our attention to these small and clinically relevant datasets: how well do standard practices for “big data” methods in predictive modeling hold up on these smaller clinical datasets? For example, split-sampling is a common approach to internal validation.
With an extremely large dataset with millions of observations, the difference between a testing sample of 20% or 25% may not matter.
But if the data set has only a few hundred observations, a careful consideration of sufficient sample size in the training set to fit a model is balanced against the need for sufficient sample size in the test set to construct a clinically meaningful confidence interval.
Similarly, the approach to missing data --- common in clinical datasets --- for such small datasets used for prediction is unknown.
The removal of complete cases is relatively costly given the small number of observations.
Finally, all of these decisions could be guided by statistical expertise and clinical insight into the problem at hand, or could be left to purely automated methods — called “automated machine learning” to use the data itself to guide analytic choices around model selection and imputation.

Therefore, using a small clinical dataset, we sought to compare different approaches to split sampling, handling of missing values, and choosing a model type across two long-term outcomes in patients with critical illness.
