## Introduction

A surge of interest in predictive modeling techniques has paralleled the increasing availability of large data sets and open source software packages that allow nearly out-of-the-box model development.
The popularity of and potential for data science methods is particularly relevant to the health care setting where decision making under uncertainty with large and varied data inputs are the daily norm.
However, many advanced modeling approaches have failed to yield evidence for their superiority over traditional statistical methods.[@doi:10.1016/j.jclinepi.2019.02.004; @doi:10.1371/journal.pone.0194889]
Comparisons between statistical and machine learning methods have focused on relatively large datasets, or "big data."
However, prospectively collected, clinically rich datasets of cohorts with relevant, patient-centered outcomes are more rare.
With less noise in the cohort selection and training labels, these prospective cohorts, albeit typically smaller due to the expense of constructing them, offer an opportunity to better isolate the effects of different modeling approaches.

However, small prospectively collected datasets present additional unique and unexplored questions.
First, how much data are wasted in using a split-sampling approach for internal validation?[@doi:10.1016/j.jclinepi.2018.07.010]
With an extremely large dataset with millions of observations, the difference between a testing sample of 20% or 25% may not matter.
But if the data set has only a few hundred observations, a careful consideration of sufficient sample size in the training set to fit a model is balanced against the need for sufficient sample size in the test set to construct a clinically meaningful confidence interval.
Second, the tradeoffs in approaches to missing data --- common in clinical datasets --- for such small datasets used for prediction is unknown.[@doi:10.1097/CCM.0000000000002706; @doi:10.1016/j.artmed.2013.01.003]
The removal of complete cases is relatively costly given the small number of observations while imputation may introduce or reinforce bias.
Third, with a small dataset, does incorporating repeated measures across a patient's trajectory improve predictive performance?[@doi:10.1016/j.resuscitation.2016.02.005]
Finally, all of these decisions could be guided by statistical expertise and clinical insight into the problem at hand, or could be left to purely automated methods -â€” called ``automated machine learning'' to use the data itself to guide analytic choices around model selection and imputation.[@doi:10.1007/978-3-319-31204-0_9]

Therefore, using a small, prospectively collected clinical dataset with six-month outcomes, we sought to compare different approaches to split sampling, handling of missing values, use of repeated measures across time, and model selection across two long-term outcomes in patients with critical illness.
