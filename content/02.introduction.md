## Introduction

A surge of interest in predictive modeling techniques has paralleled the increasing availability of large data sets and open source software packages that allow nearly out-of-the-box model development.
The popularity of and potential for data science methods is particularly relevant to the health care setting where decision making under uncertainty with large and varied data inputs are the daily norm.
However, many such modeling approaches have failed to yield evidence for their superiority over traditional statistical methods.[@doi:10.1016/j.jclinepi.2019.02.004]
Comparisons between statistical and machine learning methods have primarily focused on large datasets, or "big data."
But these large datasets are usually observational and suffer from numerous biases in the data collection process that limit their use.
Prospectively collected, clinically rich datasets with relevant, patient-centered outcomes are more rare.
With less noise in the cohort selection and training labels, these prospective cohorts, albeit typically smaller due to the expense of constructing them, offer an opportunity to better isolate the effects of different modeling approaches.

Therefore, we turn our attention to these small and clinically relevant datasets: how well do standard practices for “big data” methods in predictive modeling hold up on these smaller clinical datasets? For example, split-sampling is a common approach to internal validation.
With an extremely large dataset with millions of observations, the difference between a testing sample of 20% or 25% may not matter.
But if the data set has only a few hundred observations, a careful consideration of sufficient sample size in the training set to fit a model is balanced against the need for sufficient sample size in the test set to construct a clinically meaningful confidence interval.
Similarly, the approach to missing data --- common in clinical datasets --- for such small datasets used for prediction is unknown.
The removal of complete cases is relatively costly given the small number of observations.
Finally, all of these decisions could be guided by statistical expertise and clinical insight into the problem at hand, or could be left to purely automated methods — called “automated machine learning” to use the data itself to guide analytic choices around model selection and imputation.

Therefore, using a small clinical dataset, we sought to compare different approaches to split sampling, handling of missing values, and choosing a model type across two long-term outcomes in patients with critical illness.
